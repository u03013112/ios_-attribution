{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "031e3d99-7fe3-4846-b0af-5210ec3fa55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE data_science.default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "085a0887-c389-491a-b077-51b4a2fa11e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "尝试从目前数据中获得尽可能多的特征：\n",
    "1、24小时内的付费次数、付费总金额，最大单次付费金额\n",
    "2、48小时内的付费次数、付费总金额，最大单次付费金额\n",
    "3、72小时内的付费次数、付费总金额，最大单次付费金额\n",
    "以及最终的结果：\n",
    "168小时付费金额\n",
    "\n",
    "创建一个view来方便后续快速获取数据,lw_20250820_aos_gpir_uid_revenue_view3_by_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3f4bfbd-ae1e-4854-bd02-fb89756d27c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE VIEW lw_20250820_aos_gpir_uid_revenue_view3_by_j AS\n",
    "select\n",
    "\tuid,\n",
    "\tinstall_day,\n",
    "\t-- country,\n",
    "\tcountry_group,\n",
    "\tCASE\n",
    "\t\tWHEN mediasource = 'applovin_int'\n",
    "\t\tAND UPPER(campaign_name) LIKE '%D7%' THEN 'applovin_int_d7'\n",
    "\t\tWHEN mediasource = 'applovin_int'\n",
    "\t\tAND UPPER(campaign_name) LIKE '%D28%' THEN 'applovin_int_d28'\n",
    "\t\tWHEN mediasource IN (\n",
    "\t\t\t'googleadwords_int',\n",
    "\t\t\t'Facebook Ads',\n",
    "\t\t\t'bytedanceglobal_int',\n",
    "\t\t\t'snapchat_int',\n",
    "\t\t\t'moloco_int'\n",
    "\t\t) THEN mediasource\n",
    "\t\tELSE 'other'\n",
    "\tEND as mediasource,\n",
    "\tcampaign_id,\n",
    "  -- campaign_name,\n",
    "\tpayment_count_24h,\n",
    "\trevenue_24h as revenue_24h,\n",
    "\tmax_payment_24h,\n",
    "\tpayment_count_48h,\n",
    "\trevenue_48h,\n",
    "\tmax_payment_48h,\n",
    "\tpayment_count_72h,\n",
    "\trevenue_72h as revenue_72h,\n",
    "\tmax_payment_72h,\n",
    "\trevenue_168h as revenue_d7\n",
    "from\n",
    "\t(\n",
    "\t\tselect\n",
    "\t\t\tt1.uid,\n",
    "\t\t\tt1.install_timestamp,\n",
    "\t\t\tdate_format(from_unixtime(t1.install_timestamp), 'yyyyMMdd') as install_day,\n",
    "\t\t\tt1.country,\n",
    "\t\t\tCOALESCE(cg.country_group, 'other') AS country_group,\n",
    "\t\t\tt1.mediasource,\n",
    "\t\t\tt1.campaign_id,\n",
    "      pub.campaign_name as campaign_name,\n",
    "\t\t\tcount(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 24 * 60 * 60 then t2.revenue_value_usd end) as payment_count_24h,\n",
    "\t\t\tsum(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 24 * 60 * 60 then t2.revenue_value_usd else 0 end) as revenue_24h,\n",
    "\t\t\tmax(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 24 * 60 * 60 then t2.revenue_value_usd else 0 end) as max_payment_24h,\n",
    "\t\t\tcount(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 48 * 60 * 60 then t2.revenue_value_usd end) as payment_count_48h,\n",
    "\t\t\tsum(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 48 * 60 * 60 then t2.revenue_value_usd else 0 end) as revenue_48h,\n",
    "\t\t\tmax(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 48 * 60 * 60 then t2.revenue_value_usd else 0 end) as max_payment_48h,\n",
    "\t\t\tcount(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 72 * 60 * 60 then t2.revenue_value_usd end) as payment_count_72h,\n",
    "\t\t\tsum(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 72 * 60 * 60 then t2.revenue_value_usd else 0 end) as revenue_72h,\n",
    "\t\t\tmax(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 72 * 60 * 60 then t2.revenue_value_usd else 0 end) as max_payment_72h,\n",
    "\t\t\tsum(case when (t2.event_time / 1000 - t1.install_timestamp) between 0 and 168 * 60 * 60 then t2.revenue_value_usd else 0 end) as revenue_168h\n",
    "\t\tfrom\n",
    "\t\t\tmarketing.attribution.dws_overseas_gpir_unique_uid t1\n",
    "\t\t\tleft join marketing.attribution.dwd_overseas_revenue_allproject t2 on t1.app = t2.app\n",
    "\t\t\tand t1.uid = t2.uid\n",
    "\t\t\tLEFT JOIN lw_country_group_table_by_j_20250703 cg ON t1.country = cg.country\n",
    "\t\t\tLEFT JOIN (\n",
    "\t\t\t\tSELECT\n",
    "\t\t\t\t\tcampaign_id,\n",
    "\t\t\t\t\tMAX(campaign_name) AS campaign_name\n",
    "\t\t\t\tFROM\n",
    "\t\t\t\t\tprodb.public.applovin_campaign_info_new\n",
    "\t\t\t\tGROUP BY\n",
    "\t\t\t\t\tcampaign_id\n",
    "\t\t\t) pub ON t1.campaign_id = pub.campaign_id\n",
    "\t\twhere\n",
    "\t\t\tt1.app = 502\n",
    "\t\t\tand t1.app_package = 'com.fun.lastwar.gp'\n",
    "\t\tgroup by\n",
    "\t\t\tt1.uid,\n",
    "\t\t\tt1.install_timestamp,\n",
    "\t\t\tt1.country,\n",
    "\t\t\tCOALESCE(cg.country_group, 'other'),\n",
    "\t\t\tt1.mediasource,\n",
    "\t\t\tt1.campaign_id,\n",
    "\t\t\tpub.campaign_name\n",
    "\t)\n",
    "-- where\n",
    "-- \trevenue_168h > 0\n",
    "-- order by\n",
    "-- \trevenue_168h desc\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfd90cc2-1573-4c63-8c47-1078defd78bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 这部分代码在serverless中运行，以便于快一点。\n",
    "\n",
    "-- 删除现有VIEW\n",
    "DROP VIEW IF EXISTS lw_20250827_traindata_for_3p7_gbt_by_j;\n",
    "\n",
    "-- 创建物化表\n",
    "CREATE OR REPLACE TABLE lw_20250827_traindata_for_3p7_gbt_by_j\n",
    "USING DELTA  -- 使用Delta格式获得更好性能\n",
    "PARTITIONED BY (country_group)  -- 按country_group分区\n",
    "AS\n",
    "select\n",
    "  count(uid) as users,\n",
    "  country_group,\n",
    "  payment_count_24h,\n",
    "  ROUND(revenue_24h) as revenue_24h,\n",
    "  ROUND(max_payment_24h) as max_payment_24h,\n",
    "  payment_count_48h,\n",
    "  ROUND(revenue_48h) as revenue_48h,\n",
    "  ROUND(max_payment_48h) as max_payment_48h,\n",
    "  payment_count_72h,\n",
    "  ROUND(revenue_72h) as revenue_72h,\n",
    "  ROUND(max_payment_72h) as max_payment_72h,\n",
    "  ROUND(revenue_d7) as revenue_d7\n",
    "from lw_20250820_aos_gpir_uid_revenue_view3_by_j\n",
    "where install_day between 20250101 and 20250615\n",
    "GROUP BY\n",
    "  country_group,\n",
    "  payment_count_24h,\n",
    "  ROUND(revenue_24h),\n",
    "  ROUND(max_payment_24h),\n",
    "  payment_count_48h,\n",
    "  ROUND(revenue_48h),\n",
    "  ROUND(max_payment_48h),\n",
    "  payment_count_72h,\n",
    "  ROUND(revenue_72h),\n",
    "  ROUND(max_payment_72h),\n",
    "  ROUND(revenue_d7);\n",
    "\n",
    "-- 优化表统计信息\n",
    "ANALYZE TABLE lw_20250827_traindata_for_3p7_gbt_by_j COMPUTE STATISTICS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a5a522bf-7b87-4964-b57c-cd7d4ebfd0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select \n",
    "  users,\n",
    "  country_group,\n",
    "  payment_count_24h,\n",
    "  revenue_24h,\n",
    "  max_payment_24h,\n",
    "  payment_count_48h,\n",
    "  revenue_48h,\n",
    "  max_payment_48h,\n",
    "  payment_count_72h,\n",
    "  revenue_72h,\n",
    "  max_payment_72h,\n",
    "  revenue_d7\n",
    "from lw_20250827_traindata_for_3p7_gbt_by_j\n",
    "limit 10\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66bee34f-b406-41d4-b305-47d1e91eab29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "数据准备和探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400be96d-1d42-4949-b714-0a49ac397680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 首先准备数据\n",
    "sql = '''\n",
    "select \n",
    "  users,\n",
    "  country_group,\n",
    "  payment_count_24h,\n",
    "  revenue_24h,\n",
    "  max_payment_24h,\n",
    "  payment_count_48h,\n",
    "  revenue_48h,\n",
    "  max_payment_48h,\n",
    "  payment_count_72h,\n",
    "  revenue_72h,\n",
    "  max_payment_72h,\n",
    "  revenue_d7\n",
    "from lw_20250827_traindata_for_3p7_gbt_by_j\n",
    "where revenue_d7 is not null  -- 确保目标变量不为空\n",
    "'''\n",
    "\n",
    "df = spark.sql(sql)\n",
    "\n",
    "# 检查数据基本信息\n",
    "print(\"=== 数据基本信息 ===\")\n",
    "print(f\"总行数: {df.count()}\")\n",
    "print(f\"总列数: {len(df.columns)}\")\n",
    "\n",
    "# 查看各country_group的数据分布\n",
    "print(\"\\n=== Country Group 分布 ===\")\n",
    "from pyspark.sql.functions import count, avg, stddev\n",
    "\n",
    "country_stats = df.groupBy(\"country_group\").agg(\n",
    "    count(\"*\").alias(\"sample_count\"),\n",
    "    avg(\"revenue_d7\").alias(\"avg_revenue_d7\"),\n",
    "    stddev(\"revenue_d7\").alias(\"std_revenue_d7\")\n",
    ").orderBy(\"sample_count\", ascending=False)\n",
    "\n",
    "display(country_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1732a7b3-ca57-4d1b-810f-5a0dc4144247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "数据质量检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a7744d-6294-4261-be2f-300fe030badd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 检查空值和异常值\n",
    "print(\"=== 数据质量检查 ===\")\n",
    "\n",
    "# 检查空值\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "\n",
    "null_counts = df.select([\n",
    "    count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "print(\"空值统计:\")\n",
    "display(null_counts)\n",
    "\n",
    "# 检查基本统计信息\n",
    "print(\"\\n基本统计信息:\")\n",
    "display(df.describe())\n",
    "\n",
    "# 检查每个country_group的样本数量（确保有足够数据训练）\n",
    "min_samples_required = 100  # 设定最小样本数\n",
    "valid_countries = df.groupBy(\"country_group\").count().filter(col(\"count\") >= min_samples_required)\n",
    "print(f\"\\n样本数 >= {min_samples_required} 的country_group:\")\n",
    "display(valid_countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cfedf02-c365-404a-8eef-abdf40bc2d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12059d41-e3d5-48c4-a96c-1af0b0126688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "import time\n",
    "\n",
    "def train_gbt_model_for_country(country_data, country_name):\n",
    "    \"\"\"\n",
    "    为单个country_group训练GBT模型\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"开始训练 Country Group: {country_name}\")\n",
    "    print(f\"样本数量: {country_data.count()}\")\n",
    "    \n",
    "    # 定义特征列\n",
    "    feature_cols = [\n",
    "        'payment_count_24h', 'revenue_24h', 'max_payment_24h',\n",
    "        'payment_count_48h', 'revenue_48h', 'max_payment_48h', \n",
    "        'payment_count_72h', 'revenue_72h', 'max_payment_72h'\n",
    "    ]\n",
    "    \n",
    "    # 检查特征列是否存在\n",
    "    missing_cols = [col for col in feature_cols if col not in country_data.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"警告: 缺少特征列 {missing_cols}\")\n",
    "        return None\n",
    "    \n",
    "    # 数据分割\n",
    "    train_data, test_data = country_data.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    print(f\"训练集样本数: {train_data.count()}\")\n",
    "    print(f\"测试集样本数: {test_data.count()}\")\n",
    "    \n",
    "    # 创建特征向量\n",
    "    vectorAssembler = VectorAssembler(\n",
    "        inputCols=feature_cols, \n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "    \n",
    "    # 创建GBT回归器，使用users作为权重\n",
    "    gbt = GBTRegressor(\n",
    "        labelCol=\"revenue_d7\",\n",
    "        featuresCol=\"features\",\n",
    "        weightCol=\"users\",  # 使用users作为权重\n",
    "        maxIter=20,\n",
    "        maxDepth=5,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # 创建管道\n",
    "    pipeline = Pipeline(stages=[vectorAssembler, gbt])\n",
    "    \n",
    "    # 训练模型\n",
    "    start_time = time.time()\n",
    "    model = pipeline.fit(train_data)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"训练完成，耗时: {training_time:.2f} 秒\")\n",
    "    \n",
    "    # 预测和评估\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    # 计算评估指标\n",
    "    evaluator_rmse = RegressionEvaluator(\n",
    "        labelCol=\"revenue_d7\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"rmse\"\n",
    "    )\n",
    "    \n",
    "    evaluator_mae = RegressionEvaluator(\n",
    "        labelCol=\"revenue_d7\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"mae\"\n",
    "    )\n",
    "    \n",
    "    evaluator_r2 = RegressionEvaluator(\n",
    "        labelCol=\"revenue_d7\", \n",
    "        predictionCol=\"prediction\", \n",
    "        metricName=\"r2\"\n",
    "    )\n",
    "    \n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    r2 = evaluator_r2.evaluate(predictions)\n",
    "    \n",
    "    print(f\"模型评估结果:\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    # 返回结果\n",
    "    return {\n",
    "        'country_group': country_name,\n",
    "        'model': model,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'training_time': training_time,\n",
    "        'train_samples': train_data.count(),\n",
    "        'test_samples': test_data.count(),\n",
    "        'predictions': predictions\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f17b82-8728-4ea8-9120-2f7e3106d610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "批量训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1241be-71ad-4409-b6c1-e0eb34301f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 获取所有country_group\n",
    "countries = df.select(\"country_group\").distinct().rdd.map(lambda row: row[0]).collect()\n",
    "print(f\"需要训练的country_group数量: {len(countries)}\")\n",
    "print(f\"Country groups: {countries}\")\n",
    "\n",
    "# 存储所有模型结果\n",
    "model_results = {}\n",
    "training_summary = []\n",
    "\n",
    "# 逐个训练模型\n",
    "for i, country in enumerate(countries, 1):\n",
    "    print(f\"\\n进度: {i}/{len(countries)}\")\n",
    "    \n",
    "    try:\n",
    "        # 过滤当前country的数据\n",
    "        country_data = df.filter(col(\"country_group\") == country)\n",
    "        \n",
    "        # 检查样本数量\n",
    "        sample_count = country_data.count()\n",
    "        if sample_count < 50:  # 设定最小样本数阈值\n",
    "            print(f\"跳过 {country}: 样本数太少 ({sample_count})\")\n",
    "            continue\n",
    "        \n",
    "        # 训练模型\n",
    "        result = train_gbt_model_for_country(country_data, country)\n",
    "        \n",
    "        if result:\n",
    "            model_results[country] = result\n",
    "            training_summary.append({\n",
    "                'country_group': country,\n",
    "                'rmse': result['rmse'],\n",
    "                'mae': result['mae'],\n",
    "                'r2': result['r2'],\n",
    "                'training_time': result['training_time'],\n",
    "                'train_samples': result['train_samples'],\n",
    "                'test_samples': result['test_samples']\n",
    "            })\n",
    "            \n",
    "            print(f\"✅ {country} 训练成功\")\n",
    "        else:\n",
    "            print(f\"❌ {country} 训练失败\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {country} 训练出错: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # 每训练5个模型后显示进度摘要\n",
    "    if i % 5 == 0:\n",
    "        print(f\"\\n--- 进度摘要 (已完成 {len(model_results)}/{i}) ---\")\n",
    "        if training_summary:\n",
    "            avg_rmse = sum([r['rmse'] for r in training_summary]) / len(training_summary)\n",
    "            avg_r2 = sum([r['r2'] for r in training_summary]) / len(training_summary)\n",
    "            print(f\"平均 RMSE: {avg_rmse:.4f}\")\n",
    "            print(f\"平均 R²: {avg_r2:.4f}\")\n",
    "\n",
    "print(f\"\\n🎉 所有训练完成! 成功训练了 {len(model_results)} 个模型\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff3e1e84-18f2-4118-aebc-54188a383a26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "训练结果汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb27314c-6ba8-48cf-8590-006a79d8a0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 创建训练结果汇总表\n",
    "if training_summary:\n",
    "    summary_df = spark.createDataFrame(training_summary)\n",
    "    \n",
    "    print(\"=== 训练结果汇总 ===\")\n",
    "    display(summary_df.orderBy(\"r2\", ascending=False))\n",
    "    \n",
    "    # 计算整体统计\n",
    "    print(\"\\n=== 整体统计 ===\")\n",
    "    summary_stats = summary_df.agg(\n",
    "        avg(\"rmse\").alias(\"avg_rmse\"),\n",
    "        avg(\"mae\").alias(\"avg_mae\"), \n",
    "        avg(\"r2\").alias(\"avg_r2\"),\n",
    "        avg(\"training_time\").alias(\"avg_training_time\")\n",
    "    )\n",
    "    display(summary_stats)\n",
    "    \n",
    "    # 找出表现最好和最差的模型\n",
    "    best_model = summary_df.orderBy(\"r2\", ascending=False).first()\n",
    "    worst_model = summary_df.orderBy(\"r2\", ascending=True).first()\n",
    "    \n",
    "    print(f\"\\n表现最好的模型: {best_model['country_group']} (R² = {best_model['r2']:.4f})\")\n",
    "    print(f\"表现最差的模型: {worst_model['country_group']} (R² = {worst_model['r2']:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6095524550829790,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
